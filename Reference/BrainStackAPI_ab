prompt_templates/claude/
blueprint.md
bundle.md

-

-

prompt_templates/claude/changelog.md

-

prompt_templates/claude/

prompt_templates/codex/implement_code.md

-

prompt_templates/gemini/

seo_optimize.md

-

prompt_templates/gemini/brief_summary.md

research/injected_answer.md

-

-

prompt_templates/

prompt_templates/ops/inbox_summary.md

(for summarizing

inbox tasks) - prompt_templates/ops/task_delay_decision.md (for Claude deciding on delayed task
action)
We'll document the purpose of each template at the top of the file in a comment, and ensure tasks refer to
them. This way, non-developers or the team’s prompt designers can edit these templates without touching
Python logic – making prompt iteration faster.
Finally, we will keep track of versions of prompts if needed. Possibly just via git history or adding a version
number in a comment. If a prompt heavily influences outputs and we update it, we might want to note it in
the changelog.
The Prompt Template Architecture thus gives a library of “AI instructions” that is as important as our code.
Just as code modules can be reused, prompt templates are reused by tasks. This separation also allows
future fine-tuning: e.g., if one day we fine-tune a model or we have a different model requiring slightly
different phrasing, we could have alternative templates, selected by model name or version.
In summary, by categorizing prompts into Claude’s document-style prompts, Codex’s coding prompt,
Gemini’s SEO and summarization prompts, and injection templates for research, we ensure each agent is
prompted in the optimal way. The structure enforces consistency, so outputs of the same type always have
the same format (less surprise when integrating results). And embedding metadata in outputs (like titles,
dates, citations) ensures these AI-generated documents are immediately useful in context like publishing or
reporting, with minimal post-editing.

💻 Codex Task Blueprint
This section enumerates all the new or modified code components (Python modules and TypeScript/TSX
files) that need to be created to realize the design, serving as a "to-do list" for implementation. Each item
includes the file path (within the monorepo structure), a brief description of its purpose, and key functions
or classes to implement. This comprehensive list will guide the Codex agent (or developers) in generating
the necessary code with no guesswork.
Backend (FastAPI & Tasks) Files:

33

File Path

Purpose

Key Functions/Classes

apps/backend/main.py

FastAPI application setup and entry
point. Importantly, mounts routers,
configures middleware (rate limiting,
CORS, logging), and triggers startup
events (like DB init).

app = FastAPI(...) with lifespan;
include_router(...) for each module;

startup_event to ensure DB migrations a
template loading.

Endpoints: POST /task/run (enqueue Cele

apps/backend/routes/
tasks.py

Defines API endpoints for running
tasks and pipeline flows (non-chat).
Handles immediate or queued
execution and streaming responses.

returns task_id) 60 ; POST /task/design o

nl-design (invoke design pipeline Claude->

GET /task/status/{id} (check Celery or

status, returns result) 10 ; potentially
POST /pipeline/run (generic pipeline trig
name).

apps/backend/routes/
auth.py

Authentication endpoints (token
issuance, refresh, logout). Manages
JWT and cookies.

POST /auth/token (verify user from AUTH

and return JWT + CSRF) 97 ; POST /auth/re

(rotate JWT) 113 ; POST /auth/logout (clea
114 .

POST /memory/query (semantic search acr
12 ;

POST /memory/write (store new doc

embed) 115 ; POST /memory/update (re-em
apps/backend/routes/
memory.py

Endpoints for memory and knowledge
base operations (RAG).

updated doc) 116 ; POST /knowledge/doc/

separate, to handle file upload of docs; calls w
GET /memory/search for simple text searc

memory (calls memory_store.search with que
filters).

POST /webhook/slack/command (parse co

text and call appropriate function) 52 ;
POST /webhook/slack/event (log Slack e

memory or trigger action) 54 ; POST /webho
apps/backend/routes/
webhooks.py

Consolidated external webhook
endpoints (Slack, ClickUp, Stripe,
Make). Verifies secrets and delegates
to integration handlers or tasks.

clickup (handle ClickUp event: e.g., new tas

automation); POST /webhook/stripe (ver

signature, on payment success trigger sync_
task) 117 ; POST /webhook/make (verify

MAKE_WEBHOOK_SECRET , run incoming task

POST /webhook/notion (if using, handle N

events). These handlers likely call into integ
*.py utility functions or directly run_task

34

File Path

Purpose

Key Functions/Classes
POST /agent/inbox/approve (approve a

task) 65 ; POST /agent/inbox/reject (re

maybe not explicitly in current code but we ad
POST /agent/inbox/delay (delay task wi

time) 83 ; POST /agent/inbox/prioritiz
apps/backend/routes/
agents.py

Endpoints for agent-specific actions
and multi-agent orchestrations.

or summarize inbox) 118 ; GET /agent/inbo

pending tasks, possibly with summary counts

POST /agent/inbox/summary (force Claud

summarization of inbox, if needed). Also endp
initiate specific multi-step flows like POST /a

plan/weekly (calls weekly strategy agent) 1

similar endpoints already present which will n
LangGraph internally.
apps/backend/agents/
langgraph.yaml

YAML configuration defining the agent
graph nodes and flows (as described in
the LangGraph section).

No functions (data file). Will include node defin
type, model, etc.) and flows with edges. This fi
parsed at runtime to build the graph.

Class AgentNode (with properties like id, typ

reference or model name, etc.); Class AgentG

apps/backend/agents/
base.py

Core logic for executing the

FlowExecutor with method

LangGraph. Includes a parser for the
YAML and an executor that can run a
given flow by traversing nodes.
Manages context passing and
conditional logic.

lookup node definitions from YAML (or a pre-l
structure) and call either LLM wrappers or int
functions accordingly. Also, functions like
load_graph_config() to parse YAML on s

run_flow(flow_name, inputs) . The exe

Handles condition evaluation and possibly pa
execution if specified.

def run(prompt: str, model: str = "

apps/backend/agents/
claude_agent.py

Wrapper for calling Claude API
(Anthropic). Provides standardized
interface (e.g., taking a prompt and
returning the completion). Also might
handle streaming.

35

v1") -> str to do a single completion call
httpx to Anthropics endpoint) 121

122 ;

as

stream(prompt: str) -> AsyncGenerato

for streaming SSE tokens (if Anthropic stream
available via API, otherwise simulate chunking
keys from settings. Possibly refactor from exis
claude_prompt.py logic.

File Path

Purpose

Key Functions/Classes

def run(prompt: str, model: str = "

> str – calls OpenAI ChatCompletion API wi

apps/backend/agents/
codex_agent.py

Wrapper for OpenAI GPT-4 (or 3.5
Codex if still separate) focusing on
code generation.

prompt and system instructions (like few-shot
Ensure it respects tokens limit and format (po
instruct model to output JSON if needed). as

stream(prompt: str) -> AsyncGenerato

needed for streaming code (though likely we
and return full code because partial code isn't
without all context).
def run(prompt: str, model: str =

"gemini-1.5-pro") -> str – posts to Go

apps/backend/agents/
gemini_agent.py

Wrapper for Google Gemini model
calls. Uses the PaLM API
(GenerativeLanguage API) as in current
gemini_prompt.py 123 124 .

current code does (with API key) 125 , handles
and returns content. Possibly handle chunkin
too long (1000-token limit maybe). We might
specialized calls like a
def embed_text(text: str) -> List[f
Gemini or PaLM offers embedding (if not, we
OpenAI for that, which belongs elsewhere).

def run(query: str) -> dict – for now

just log the query and return {"status": "queue
perplexityRelay.ts stub 126 . Ideally, int

apps/backend/agents/
search_agent.py

apps/backend/tasks/
__init__.py

Logic for performing web searches or
calling Perplexity API. If no official API,
this might call our own headless or use
an alternative (maybe we use an SERP
API or a local knowledge base of recent
data). However, given the mention of
Perplexity, likely a function that
triggers an external process or uses a
stored search mechanism.

Perplexity’s API if available or use a service lik
Google Custom Search to get top results, then
LLM summarize. Perhaps break into two:
search_web(query) -> list[Source] ,

Task registry initializer. Scans all task
modules and registers them (similar to
current
brainops_operator._load_tasks )

On import, execute function to import submo
call register_task . Possibly maintain

4

.

summarize_sources(sources) -> str .

Claude or GPT to summarize multiple results.
node might thus orchestrate internally (calls s
then calls an LLM agent to summarize) unless
that to the graph definition (which could inste
two nodes: search and Claude). For now, a sim
implementation: use some search client (if no
placeholder).

_TASK_REGISTRY dict mapping task_id to fu

may not need a dataclass like TaskDefiniti

table stores metadata, but keep for quick look

36

File Path

apps/backend/tasks/
autopublish_content.py

Purpose

Key Functions/Classes

Example of a composed task that might
use multi-agents under the hood to
publish a blog or product content.
(From README: publish an article to
site, trigger Make uploads, send
newsletter) 127 . We might
reimplement it to use our new
pipelines: e.g. uses Claude to draft
content, maybe Codex to format HTML,
etc., then calls integration to Make or
direct API.

TASK_ID = "autopublish_content" ; de

run(context) – parse context (article detail

call agents.base.run_flow("publish_co

context) if we define that flow, or manually

orchestrate steps: e.g. call Claude for draft
( claude_agent.run(template="blog_pos

fields={...}) ), call an integration to publi

(maybe using requests to a WordPress or D

then return result. This task demonstrates usa
multiple components.

run(context) – context might have produ

apps/backend/tasks/
generate_product_docs.py

Task to create product documentation
with Claude and push to
documentation site. Possibly uses a
prompt template (Claude) and then
uses an integration (like calling GitHub
or an API to commit the doc).

doc format needed. It will use something like
claude_agent.run(template="product_

fields={...}) , get markdown output, then

integrations.notion.publish_page(ou

if docs site is in git, create a file via GitHub AP
we might have an integration). Return succes
doc.

run(context) – read JSON from context (m
apps/backend/tasks/
parse_eagleview_report.py

Task that parses a JSON (EagleView roof
report) into a CSV. This might not
involve AI at all (pure Python logic).
We'll implement it as standard code.

context has file path or JSON string), parse fie
compute quantities, output a CSV file content
structured data. No external calls. Ensure this
fairly quickly within Celery. Return perhaps a l
data (maybe store file in Supabase storage or

Task that calculates material and labor
costs from roof quantities (again more
deterministic, but could involve some
AI if we want narrative). Likely
straightforward calculation given some
parameters or using known rates.

type, pitch, etc. The task fetches standard pric
config or memory (maybe stored in docs or an
environment), multiplies out costs, and return
breakdown (could return as structured dict or
formatted Markdown/CSV). Possibly it could c
to format the estimate nicely in a paragraph,
heavy lifting is arithmetic.

run(context) – context might include area

apps/backend/tasks/
generate_roof_estimate.py

apps/backend/tasks/
claude_prompt.py

(Refactored) A generic task to prompt
Claude with given input. We might not
need this as a task separate from agent
wrapper, but current code has it to
provide a way to call Claude via task
queue or CLI 128 . We'll keep a simple
one for debugging.

37

TASK_ID = "claude_prompt" ; run(cont

expects context["prompt"] and optionall

context["template"] , calls claude_age
with that and returns completion 122 . Useful
Claude responses or making it accessible via
run .

File Path

apps/backend/tasks/
nl_task_designer.py

Purpose

Key Functions/Classes

A task that takes a natural language
goal and generates a sequence of
tasks (essentially, the initial step for
auto-generating an automation
pipeline). The current system had
something similar (maybe this is how
/task/nl-design is supposed to
work). We'll implement it to utilize
Claude or GPT-4 to output structured
tasks.

TASK_ID = "nl_task_designer" ; run(c

– context has a goal description. The task u

prompt like "Given the goal, list a set of tasks
to achieve it." Possibly uses the chat_to_pr

similar chain present (the current code used
chat_to_prompt to suggest tasks from a u

message 129 ). We might directly call Claude w
special template to generate JSON: e.g. tasks
of {task: ..., depends_on: ...}. Then return that
further processing (like automatically creating
tasks in inbox).
TASK_ID = "ai_inbox_summarizer" ;
run(context) – context might include the

apps/backend/tasks/
ai_inbox_summarizer.py

Task to summarize pending tasks in
the inbox (used for notifications when
many tasks waiting) 130 . We will keep
this to offload summarization.

fetch pending tasks within. Calls Claude or Ge
produce a summary sentence like "Tasks: X (d
W)...". Already in current code uses
claude_prompt.run with "Summarize inbo
We'll adjust to use a proper prompt template
summarization.

apps/backend/tasks/
claude_output_grader.py

Possibly a task to review or grade
Claude's output (maybe in the works to
have GPT critique itself). If not needed
immediately, skip or implement if
referenced in issues. If needed: could
have Claude or GPT-4 evaluate quality
of outputs (for QA pipeline).

If implementing: TASK_ID =

"claude_output_grader" ; run(context

has some output to grade and criteria. It retu
or feedback. This might be used internally to
run needed.

run(context=None) – checks current time

apps/backend/tasks/
recurring_task_engine.py

Manages recurring tasks scheduling
(reads a list of schedules and enqueues
tasks when due) 93 94 . We'll expand
if needed or keep as is but integrated.
Possibly convert storage to DB (but can
keep JSON for simplicity).

recurring_tasks.json , for each due, call

agent_inbox.add_to_inbox to queue it (

'recurring') 131 . Functions:
add_recurring_task(entry) to add to JS

save 132 (wired to an API). We might add
remove_recurring_task or an update fun
completeness.

38

File Path

Purpose

Key Functions/Classes

run(context=None) – loads delayed_ta
Handles delayed tasks and escalation
133 71 . We will use this to process
tasks that were postponed. It uses
Claude to decide action. We'll keep the
logic, maybe adjusting to mark
statuses in the tasks table instead of

apps/backend/tasks/
task_rescheduler.py

JSON where possible.

(Possibly referenced by endpoint /
memory/audit/diff

135 ). Could be a

apps/backend/tasks/

task to find differences between

memory_diff_checker.py

memory states, maybe for debugging.
If in scope, implement a simple diff on
memory logs; otherwise, deprioritize.

checks each if due 134 , for due ones calls Clau
claude_prompt.run ) with prompt asking e

close/rerun 71 . Then depending on decision,
task (calls run_task synchronously or enqu

mark as closed or escalate (update status). Alr
agent_inbox.mark_as_resolved or update stat
accordingly 72 . We might adapt escalate to ju
pending but mark differently (maybe "escalat
triggers human attention in UI).

If implemented: would fetch two sets of mem
(maybe from two time ranges or sessions) an
diff (which could be done by a git-diff library o
comparing text). Not critical for core functiona
possibly skip unless needed.

Integrations & Utilities:
File Path

Purpose

Key Functions/Classes
def send_message(text: str,
channel=None) – posts to Slack incoming

apps/backend/
integrations/
slack.py

Utilities for sending messages to
Slack and verifying Slack
requests. Already present as
utils.slack in current code
77

78 . We maintain here.

webhook or bot (using
SLACK_WEBHOOK_URL for simplicity as in
current code 78 , which sends errors to
Slack). verify_request(request) – use
SLACK_SIGNING_SECRET to validate
signature in headers (Slack sends timestamp
and signature, we combine and compare
hash). Also parse Slack command payload
format.

39

File Path

Purpose

Key Functions/Classes

Functions to call ClickUp API
(create task, update task) and
perhaps handle incoming
webhook payload. The code
already hints with
create_clickup_task and

title, description, token) – uses

update_clickup_task in

etc. def handle_webhook(payload) –

clickup_adapter 136 . We'll
implement these properly with
ClickUp API endpoints.

parse incoming JSON from ClickUp webhook
and return a context or call a task (like if a
task moved to "To Automate" list, trigger
something).

def create_clickup_task(list_id,

apps/backend/
integrations/
clickup.py

ClickUp API (POST to /api/v2/list/
{list_id}/task ) with given token. Returns
created task ID. def
update_clickup_task(task_id, fields,
token) – PUT to update name/description

def create_page(title,
content_markdown, parent_db) – create

apps/backend/

Functions for Notion integration.
Possibly create/read Notion
pages or databases. If we have
an API key ( NOTION_API_KEY ,

integrations/

NOTION_DB_ID in settings 137 ),

notion.py

we can use the official Notion
SDK or direct HTTP. Use-case:
maybe pushing generated docs
to Notion.

apps/backend/
integrations/
make.py

A simple handler for Make.com.
Likely just verifying a secret and
packaging data to feed to tasks.
Make usually can send whatever;
here we design expecting maybe
task_name and context .

a new Notion page (with given parent
database or page ID) and fill with content
(Notion API requires structured blocks; might
need a Markdown->blocks conversion or
simple if only text). def
update_page(page_id,
content_markdown) similarly. If receiving
webhooks from Notion, def
handle_webhook(payload) – likely not,
since Notion doesn’t send outbound
webhooks easily; we may rely on polling if
needed (or just user manual trigger).
def handle_webhook(request_data) –
e.g., if request_data has task and
context , simply do run_task(task,
context) . Could allow multiple tasks or
some branching if needed. The endpoint logic
might suffice without separate function, but
we place here for clarity.
def handle_webhook(request) – get
headers and body, use stripe.Signature

apps/backend/
integrations/
stripe.py

Verify Stripe webhook signature
and extract event data to call
tasks. Possibly use Stripe’s library
if available, or manual HMAC
check on payload using
STRIPE_WEBHOOK_SECRET .

if using library, or manually construct
signature to compare. Determine event type;
if checkout.session.completed , call
run_task("sync_sale",
{"session_id": ...,
"customer_email": ...}) . If
invoice.paid or others, maybe handle
similarly or ignore.

40

File Path

apps/backend/
core/
settings.py

Purpose

Key Functions/Classes

Already defined environment
settings using Pydantic 138 , we
may extend to include any new
config (like model names, or
feature flags for multi-agent
mode).

Add fields if needed: e.g. OPENAI_MODEL
default, ENABLE_CHAIN_MODE . But largely
we use existing keys: CLAUDE_API_KEY ,
OPENAI_API_KEY , GEMINI_API_KEY , etc.
Possibly add e.g. PERPLEXITY_COOKIE or
similar if needed to call it. Ensure
model_config has correct env file path.
Could define an async def
scheduler_loop() that runs in background

apps/backend/
core/
scheduler.py

A new module to manage
background jobs (cron-like tasks).
If we don’t use Celery Beat, we
can implement a lightweight
scheduler using asyncio loops
on startup.

(FastAPI lifespan event) checking every
minute for tasks to run: call
recurring_task_engine.run() , call
task_rescheduler.run() , etc. Or use
Celery periodic tasks (in celery_app.py
currently might be configured, but simpler is
to have our own loop or rely on an external
cron hitting certain endpoints). We'll likely
implement at least a basic internal scheduler
for recurring tasks (since we store them in
JSON, easier to manage directly).
def hash_password(password) , def

apps/backend/
core/
security.py

Utilities for hashing passwords
(using passlib pbkdf2 as in
139 ), verifying JWT tokens,

main
etc. Possibly refactor logic from
main into here.

verify_password(password, hash) . JWT:
def create_access_token(sub, roles,
expires_minutes) and def
decode_token(token) . Right now, main
does JWT encode/decode inline 140 – we can
move that here for cleanliness.
def configure_logging() – remove

apps/backend/
core/
logging.py

Set up loggers, sinks (Slack,
Supabase). Already partly in main
where it adds Slack and supabase
sinks to loguru logger 78 141 .
We modularize that.

Frontend (React/Next.js) Files:

41

default handlers, add JSON stdout, add
_slack_sink for level ERROR (posting to
Slack) 78

141 , add

_supabase_sink if

supabase available (inserts log into logs
table) 142 143 . This allows tracking errors in
Supabase as well for later analysis. Possibly,
integrate with an APM if decided (not in scope
here).

File Path

apps/frontend/
brainops-admin/
pages/dashboard/
index.tsx

Purpose
Admin
Dashboard home
– an overview
page with charts
and recent
activity.

Key Components/Functions
Use Charts component to display metrics (e.g.,
tasks executed today vs week). Use TaskList to
maybe show last 5 tasks or pending tasks summary.
Possibly a welcome message or quick actions (buttons
like "New Task", "Run Daily Plan"). Fetch data from e.g.
/dashboard/metrics for counts 144 or directly
from tasks endpoints.
Fetch pending tasks from /agent/inbox (which can

pages/dashboard/

Inbox page
showing pending
tasks needing

inbox.tsx

approval.

apps/frontend/
brainops-admin/

return list of tasks with summary) 63 . Render list with
each task name/summary, and "Approve / Reject /
Delay" buttons. On approve, call /agent/inbox/
approve with task_id 65 ; on reject, maybe call a
similar endpoint (or we treat reject as mark resolved
with status 'rejected'); on delay, open a modal to pick
time and call /agent/inbox/delay 83 . Use
WebSocket or polling to update list when changes
occur (or optimistic UI updates removing approved
task).

apps/frontend/
brainops-admin/
pages/dashboard/
tasks.tsx

Page listing all
tasks (or perhaps
tabs for active,
completed,
failed).

Call /tasks?status=... (we might implement a
query or just fetch all from /dashboard/full if
exists 145 ). Display in a table with pagination if large.
Columns: ID, name, status, user, created_at,
completed_at. Perhaps color-code status. Each row
clickable to see details.
Fetch /tasks/status/{id} for current status and

apps/frontend/
brainops-admin/
pages/dashboard/
tasks/[id].tsx

Task detail page,
showing context
and output of a
specific task, plus
logs.

result 10 , and maybe another endpoint for full detail
(if context is large, might store in DB and we retrieve).
Show context (could JSON.stringify nicely or formatted
if known fields). If output is text or markdown, render
via MarkdownViewer . If it's an error, show error
stack trace. Also display logs: we could call /logs/
task/{id} if we had, or reuse tasks.result if it
contains error info. Include a "Re-run task" button
maybe (which could enqueue a new one with same
context).

42

File Path

Purpose

Key Components/Functions
Provide a search bar. On submit, call /memory/
search with query and optional filters (date range,

apps/frontend/
brainops-admin/
pages/dashboard/
memory.tsx

Interface to
search and
browse memory
logs.

tags, user). Display results as a list of snippets: e.g., if
result is memory entries, each might have output
or input text. Use MarkdownViewer for any that
are markdown. If result includes metadata like source
or tags, display those. Possibly allow filtering by tag
via checkboxes (e.g., chat vs task vs error). If nothing
entered, maybe list recent memory (last N entries
from /memory/search?limit=50 ).
Fetch list from /knowledge/doc/list (we may

List all knowledge
documents in the
apps/frontend/
brainops-admin/
pages/dashboard/
docs.tsx

system’s
knowledge base
(the docs table).
Allow upload new
doc and search
within docs.

implement or use /memory/query with no query
but project filter). Display titles and maybe first line.
Provide upload form (file or copy-paste): on submit,
call /knowledge/doc/upload with file content and
metadata; refresh list when done. Each doc item
clickable to view details. Also a search input that
specifically queries docs: calls /memory/query with
project_id if needed (or an endpoint that returns
doc search results in context).
Fetch /knowledge/doc?id=... (we may add an

apps/frontend/
brainops-admin/
pages/dashboard/
docs/[id].tsx

Document detail
page, showing
full content of a
knowledge doc.
Also possibly
allow editing and
re-saving
(update).

endpoint to get full content by id, or the list could
have content truncated). Render content via
MarkdownViewer . If editing allowed for admins:
either a simple textarea with markdown or maybe a
rich text (for now, maybe markdown editor). On save,
call /memory/update with doc_id and new content
116 , then re-render (the backend will re-embed it

asynchronously or in that call). Show list of related
docs or an option to delete doc (call an endpoint to
remove doc and its embeddings).

43

File Path

apps/frontend/
brainops-admin/
pages/dashboard/
agents.tsx

Purpose
(Optional) If we
want to visualize
or let admin
tweak the agent
graph. Could
show the
LangGraph
structure (nodes
and flows). Might
not be core, but
could be readonly view for
trust.

Key Components/Functions

If implemented: fetch langgraph.yaml (maybe the
backend can serve it as JSON via an endpoint). Display
nodes (maybe a simple list or network graph using a
library). Possibly highlight active flows. Not a priority
though – could skip in initial pass.

This depends on what we allow to change at runtime.
Could show the recurring_tasks list: fetch from

apps/frontend/
brainops-admin/
pages/dashboard/
settings.tsx

apps/frontend/
brainops-admin/
components/
TaskList.tsx

Admin settings
panel for
configuring
system
parameters (like
thresholds,
schedules, user
management).

Reusable
component to
display a list of
tasks (with
minimal info).
Could take props
like tasks array
and a subset of
fields to show.
Used on
dashboard home
and tasks page.

an endpoint or directly from JSON via an endpoint that
returns it. Provide a form to add a new recurring task
(task_id, frequency, time). Also list existing with
enable/disable or remove. Could show current
environment flags (like which models keys are
configured). For user management, list users (from
env or an in-memory store since we have
AUTH_USERS as env – might not be editable from UI
easily unless we allow adding to environment via
secrets endpoint). This could also hold a form to send
test notifications or flush logs, etc. We must be
cautious exposing too much; maybe keep minimal:
recurring tasks and a link to supabase (since DB
changes beyond that likely via code).

Renders a table or list of tasks. If used on home,
perhaps only incomplete tasks or recently completed.
Accept onClick handler for row (to view detail).
Possibly use an icon or label for status (like a green
check, red x, etc.).

44

File Path

Purpose

A panel for
inputting a
prompt to the AI
apps/frontend/
brainops-admin/
components/
PromptPanel.tsx

(Claude/GPT/
Gemini) along
with controls.
Admin version
may allow
selecting which
model or chain to
use.

Key Components/Functions
Contains a textarea for the prompt text. Might have a
dropdown or toggle for model (e.g., “Claude vs GPT-4
vs Gemini” vs an "Auto" setting). Also possibly a
dropdown for mode (Chat vs Design vs Execute Task).
But perhaps simpler: admin can pick from available
flows: e.g. "Brainstorm", "Ask Data", "Dev
(Claude+Codex)". This selection could adjust how we
call backend (maybe hitting different endpoints or
including a parameter). On submit, it calls the
appropriate endpoint ( /chat if just Q&A, or /task/
nl-design if design mode, etc.) with
stream=true . It then creates an OutputStream
component below to display result. It should also
handle any suggested tasks from the response (if
ChatResponse.suggested_tasks comes back
89 ). If suggestions exist, display them as clickable

chips " Deploy update" etc., and on click, maybe open
a modal to confirm adding to Inbox or run
immediately.

45

File Path

Purpose

Key Components/Functions
Use React state to accumulate content . On mount,
open EventSource to a given URL (passed via props
or if we use fetch that yields a readable stream, but
SSE is simpler in browser). For /chat streaming,
backend yields data: token lines 146 . We parse
and append token to content. If the event stream
closes, we finalize. Also possibly handle
suggested_tasks which might not come until the

apps/frontend/
brainops-admin/
components/
OutputStream.tsx

apps/frontend/
brainops-admin/
components/
MarkdownViewer.tsx

Component to
display streaming
output from the
backend. It
connects to an
EventSource (SSE)
and appends
tokens as they
arrive. Also
indicates when
done or if error.

Renders
Markdown
content as HTML
in a safe way.

stream is done (in current logic, suggestions are only
available after full completion 147 ). But since backend
does memory_store.save_memory in finally with
suggestions, maybe suggestions are not pushed via
SSE. Alternatively, we could adapt to send a final SSE
event with suggestions encoded (like a JSON blob).
Simpler: after stream ends, do one fetch to /chat/
to-task with the user message to get suggestions
(the system already has an endpoint for chat to tasks
conversion 148 ). But the code already calls
chat_to_prompt internally and included
suggestions in ChatResponse for the non-stream case
88 . For streaming case, in our design maybe we
close SSE and then call an endpoint to get
suggestions. We'll handle as needed. The component
should scroll as content grows (like auto scroll to
bottom). Possibly allow copying the output. If output
contains markdown, we could either display raw text
(and let MarkdownViewer interpret it after done) or
even live-interpret partial markdown – probably
simpler to display raw during streaming then replace
with formatted once done. For chat, raw text is fine.
For other outputs that are code or multi-part, might
need special handling if needed (e.g., if an output is
code to be saved, we might not stream it but deliver at
once).
Likely use a library like react-markdown or Marked to
parse and render Markdown content. Enable plugins
for code syntax highlighting, etc. We should also
handle if the markdown includes HTML or dangerous
stuff; use allowed list of tags or trust it if from our AI
(should be fine mostly). Ensure it has styling (maybe
via Tailwind prose classes). This component is reused
anywhere we need to show markdown (knowledge
docs, task outputs that are markdown, etc.).

46

File Path

apps/frontend/
brainops-admin/
components/
Charts.tsx

Purpose
Displays charts
for metrics. Could
use e.g. Chart.js
via react-chartjs.
Render possibly
multiple small
charts: tasks per
day line chart,
success vs fail
donut, etc.

Key Components/Functions
Functions to transform metrics data from API to chart
datasets. If /dashboard/metrics returns JSON
counts 81 (like tasks_executed, tasks_failed,
memory_entries, etc.), we can display key numbers as
KPIs. If we want timeseries, might require the backend
to give a series (we might instead query tasks with
grouping by day, perhaps out of scope initially).
Possibly just showing cumulative counts and maybe
tasks by status pie from the tasks list. We will start
simple due to time. The component encapsulates the
library details.
function apiGet(url) wraps fetch GET with

apps/frontend/
brainops-admin/
utils/apiClient.ts

Utility for calling

Authorization: Bearer token if token in

backend API with
proper headers
(auth and CSRF)
and handling SSE
if needed.

storage and include credentials: 'include' for
cookies if needed (since we set JWT as cookie
perhaps). Similarly apiPost(url, data) . Also a
helper openEventSource(url, onMessage) for
SSE connections. Possibly handle refresh token logic if
401.
function login(username, password) calls /
auth/token and if success, stores token (maybe in

apps/frontend/
brainops-admin/
utils/auth.ts

Manage
authentication
state in the frontend. e.g.,
retrieving token
from cookie or
localStorage, and
login flow.

memory or cookie is already set HttpOnly by backend
which is safer). If we rely on HttpOnly cookie, then
apiClient should send those cookies
automatically. We might not even need to expose
token to JS. But we do need CSRF token for modifying
requests (provided by /auth/token response as
csrf_token in JSON 149 and also set as cookie by
backend). We can store that CSRF token in a JS variable
for later use (e.g., set as header X-CSRF-Token via
fetch). So auth util can after login store CSRF token in
e.g. sessionStorage. Also include logout() that
calls /auth/logout and clears state. Possibly
useAuth hook for React to protect routes, but since
Next can do server-side redirect if no token cookie,
simpler to handle on client mount.

47

File Path

apps/frontend/
myroofgenius/pages/
index.tsx (or /
copilot.tsx)

Purpose

Key Components/Functions

Main user
interface for
MyRoofGenius
Copilot. Likely a
combined chat +
form interface or
at least an
introduction and
a chat start
button.

Could show a welcome message ("Hi, I'm the Roof
Genius Assistant! Ask me anything about your roof or
request a report.") and then an input for a question.
Possibly directly embed the PromptPanel and
OutputStream but simplified. Also maybe quick action
cards like "Get a Roof Estimate" that navigates to /
estimate page.

Contains input fields (area, roof type, etc.) or a file
upload for EagleView JSON. On submit, calls the
backend: possibly first parse_eagleview_report if
file provided, then generate_roof_estimate (or

apps/frontend/
myroofgenius/pages/
estimate.tsx

apps/frontend/
myroofgenius/pages/
guide.tsx (optional)

apps/frontend/
myroofgenius/pages/
about.tsx etc.

A form
specifically to
generate a roof
estimate. It might
allow user to
upload a report
JSON or enter
dimensions
manually.

Possibly a
knowledge base
Q&A page if endusers can query
stored docs (like
a FAQ powered
by the AI).
Static pages for
marketing
(About, Contact).

we have one endpoint that orchestrates both). We
might create an endpoint /task/webhook or similar
that accepts a generic trigger with two tasks
sequentially 150 . Alternatively, we call /task/run
twice and show intermediate result. But better to
encapsulate logic server-side. For user simplicity, one
click yields final output (maybe a download link or a
nicely formatted cost breakdown on screen). The page
shows a loading indicator while processing (subscribe
to status via SSE or poll). Once done, display results:
we could present a table of materials and costs, plus a
summary sentence. Maybe also allow downloading
CSV or PDF (perhaps the tasks returns a URL for a CSV
in storage or includes CSV as text).
Provide a search or question box. When submitted,
call a backend route that does RAG Q&A on user's
behalf (maybe the same /chat but restrict model to
use knowledge base). The result is shown, along with
citations if any (could show sources from the answer if
formatted with [1][2]). This is like a self-help portal. If
not needed, skip implementing.
Mostly static content or a simple contact form (which
could call our backend /api/contact that sends to
Make webhook as per README 151 ). Likely these
come with the template or we create simple content.

48

File Path

apps/frontend/
myroofgenius/
components/
ChatBox.tsx

Purpose

Key Components/Functions

A simplified chat
interface for user.
Combines a text
input and
message display.
Unlike admin,
user gets a oneon-one
conversation
style feed (with
user and
assistant
messages).

It will manage a conversation state (list of Q&A). On
user submit, it appends user message to list, then
calls backend (likely /chat with their message and
perhaps a fixed model or a session_id tied to user if
we want context retention). It handles streaming the
assistant response (similar to OutputStream, but we
may integrate it here for simplicity). Once done, it
appends the assistant answer to chat. This chat might
also automatically incorporate RAG (the backend's /
chat already includes memory and knowledge
search). The UI shows each message in a bubble
format. Also might allow resetting the conversation
(new session id).
Handles the fields and file upload, and on submit
triggers the appropriate backend calls. Possibly
separate from page logic for reusability. It might
either directly call an API that handles everything, or
piecewise: if file present, first upload to /memory/
relay or a special endpoint to store file content

apps/frontend/
myroofgenius/
components/
EstimateForm.tsx

The form
component used
on estimate page
for input.

(maybe we should add an endpoint to accept file and
run parse in one go). We may have to handle file
reading in JS (FileReader to JSON string) and include in
context. Simpler: instruct user to obtain JSON (maybe
not realistic; better to accept file). We'll likely add in
backend something like /webhook/eagleview just
for this use-case, or extend /task/run to accept a
file by first uploading to memory and referencing it.
For now, implementing in UI: if file provided, do one
request to upload file to our backend (we have /
voice/upload example, we could similarly have /
file/upload for general files which stores in
Supabase storage or memory log), then get an ID or
content and include that in the context for tasks.
Could also do FileReader in browser to get text and
send in JSON (embedding raw file content in JSON is
possible if not huge). We'll decide based on expected
file size. Possibly simpler: instruct user to paste JSON
text into a textarea. We can accept moderately large
JSON text (a couple thousand lines maybe). For first
iteration, that might be acceptable.

49

File Path

Purpose

Key Components/Functions
For example, if generate_roof_estimate returns

apps/frontend/
myroofgenius/
components/
ResultDisplay.tsx

A component to
show results of
estimate or
similar in userfriendly way
(graphical or
structured).

a structured breakdown (like a dict of items with
costs), we can format a nice table: "Material X: $Y,
Labor: $Z", etc., and a total. Or if it returns markdown
text, just render via MarkdownViewer. Provide a
"Download CSV" button if CSV data available (e.g., if
parse task provided CSV). That button could create a
blob from CSV string and use
URL.createObjectURL to download, or if backend
gives a file URL, just link it.

apps/frontend/
myroofgenius/utils/
apiClient.ts

apps/frontend/
myroofgenius/utils/
format.ts (optional)

Similar to admin's
apiClient but
possibly a bit
different if
authentication is
different. If the
user side is open
access (no login),
we might not
need JWT except
maybe for ratelimiting. If any
user auth (maybe
for internal sales
team), we might
reuse the admin
auth system with
separate user
credentials. We'll
plan as if open
for now.
Helpers to format
data for display
(e.g., currency
formatting for
cost, etc.).

If open: just handle base URL and common headers
(like Content-Type: application/json ). If auth
needed (the user might log in to see say past reports),
then similar to admin but perhaps restricted scope.
Possibly separate environment config for user app
(NEXT_PUBLIC_API_BASE used here too).

formatCurrency(amount) etc., to ensure
consistency in displays like estimate results.

Deployment & CI Files:

50

File Path

Purpose

Key Configurations
Steps: on push to main, checkout code, set up
Node (for frontend) and Python (for backend)
environment. Run turbo run lint && turbo
run test to lint and test all apps. Possibly run
pytest for backend (assuming tests exist). Build

.github/workflows/
ci-cd.yml

Docker image for backend (or use heroku/Render
deploy) and deploy to Render (could authenticate
via Render API or use a GitHub integration). For
frontend, either build and deploy to Vercel (if
using Vercel, linking repo) or build static and
publish to S3/Netlify. Alternatively, separate
workflows for backend and frontend. Also perhaps
a job for syncing prompt templates to DB (if
needed) or running migrations. Include secrets in
GitHub for API keys (though likely these stay in
Render/Vercel). This file ensures any Claude/Codex
generation artifacts (like if we use an AI step in CI,
which is unlikely for now) are integrated.

GitHub Actions
pipeline automating
testing and
deployment of both
backend and
frontend.

Define service: likely name: brainops-

Render.com
configuration if

render.yaml (in
repo root or under
infra/)

backend , plan: free or starter, env:
python , buildCommand: pip install -r

needed (though they
might allow using
Dockerfile directly or
web settings). The
README mentions a
provided render.yaml
152 . We'll include it to
specify the
environment variables
and startup
commands for the
backend service.

requirements.txt && alembic upgrade
head , startCommand: uvicorn
apps.backend.main:app --port 10000
(matching how to run). Mount secrets for API keys.
For static dashboard, either have backend serve it
(we already do by copying build to static/) or
deploy the static as a web service (but since admin
can be served by backend at /dashboard/ui ,
we do that). We may also define a CRON job on
Render if needed to call our endpoints for
recurring tasks (if we didn't implement internal
scheduler fully – but we plan to).

51

File Path

vercel.json (if
deploying user
frontend to Vercel)

Dockerfile (for
backend)

dockercompose.yml (for
local dev)

Purpose

Key Configurations

Configuration for
Vercel deployment of
Next.js app(s). It might
specify rewrites if
needed or
environment. Possibly
not needed if Vercel
auto-detects. If user
app uses some
serverless function for
contact form, might
configure that.

Might contain routes for any backendless
functions, but likely our user app will call the main
backend for everything, so no need special. If
hosting multiple apps, might have to define each.
Alternatively, we host admin also on Vercel (but we
prefer static via backend). The user app likely can
be static or minimal SSR. We'll skip heavy config
unless necessary.

Dockerfile to
containerize FastAPI
app with all
dependencies and
runtime. Provided in
repo root (we saw one
in repo). Likely mostly
fine but we ensure it's
updated with new
structure.

Use a Python base (slim), copy code, install
requirements, set ENV UVICORN... , etc. If we
also containerize frontend, might separate or let
Vercel handle frontend. Probably simpler: one
Docker for backend including static admin UI. The
user UI could be static on Vercel or built and
served under a different path. Perhaps just let
Vercel handle user UI for scale and domain
(myroofgenius.com).

Compose file to run
backend, possibly a
Postgres (for
Supabase emulator or
direct PG), and maybe
a Redis for Celery in
dev. Also could run the
Next.js dev servers if
needed.

Services: db (Postgres with pgvector extension
loaded), maybe adminer or pgweb for DB UI, redis
for Celery broker, backend (build from Dockerfile
or use volume mount for live reload if using
uvicorn reload), and optionally a service for
supabase or vector DB if needed. Not strictly
necessary if using Supabase cloud in dev, but
good for offline. This helps new devs set up
quickly.
For example: test that
memory_store.save_memory inserts into

tests/ (various
backend tests)

Python tests to verify
critical functionality
(we likely extend
tests/

supabase properly (could mock supabase), test
recurring_task_engine schedules tasks
correctly, test prompt templates produce expected
structures (maybe by unit testing
template_loader or small prompts). Also

test_basic.py to
cover multi-agent
flows etc.).

integration tests for endpoints (using FastAPI
TestClient, e.g. test /task/run returns task_id and /
task/status eventually gives result). Given
complexity, focus on at least core logic tests.

52

File Path

sdk/index.ts &
sdk/services/
DefaultService.ts

Purpose

Key Configurations

TypeScript SDK for
interacting with our
API – perhaps used by
the Next.js apps or
possibly offered for
third parties. It's listed
in repo. We'll update if
needed to reflect new
endpoints.

Probably have classes or functions mapping to
each endpoint (like getTasks(),
runTask(task, context) etc.). If admin and
user apps are within monorepo, they could import
directly rather than use network – but since API is
separate service, better to call via HTTP. SDK might
not be heavily needed internally, but if it exists,
update endpoints accordingly.

Each of these files will be implemented following the blueprint specified. The objective is to have a clear
mapping from design to code artifacts: for example, when adding the multi-agent orchestrator, we know to
create base.py and langgraph.yaml ; when exposing new endpoints, we know exactly which route file
to edit or create. This structured task list ensures that Claude (for writing documentation and templates)
and Codex (for generating the code files) have an exact blueprint to follow.
By breaking the implementation into these discrete files and responsibilities, multiple agents (or
developers) can work in parallel. For instance, Codex can generate the FastAPI router code while another
instance generates the React components, all based on this synchronized plan. We have listed even small
utility files to leave nothing ambiguous. The end result will be a cohesive system where every component
aligns with a documented purpose, and the chances of missing functionality are minimized.
Finally, after code generation, we will perform thorough testing (both automated and manual) to validate
that each piece interacts correctly: e.g. creating a task via UI goes through API to Celery to execution and
returns result to UI; multi-step pipelines produce the intended outcomes; memory search returns relevant
info, etc. This blueprint and task list serve as the contract for that implementation phase.

CI/CD & Deployment System
To ensure smooth and reliable releases of the BrainStackStudio platform, we will set up a CI/CD pipeline
and deployment architecture that covers testing, security, and automated deployment to our chosen
hosting services (e.g. Render for backend, Vercel for frontends). The CI/CD process will tie together Claude’s
documentation outputs and Codex’s code outputs by verifying that everything is consistent and passing
tests before deploying.
Continuous Integration (CI): Every change to the repository (especially to the prompt templates or code)
will trigger a GitHub Actions workflow: - The workflow will run on pull requests and merges to main. Install & Lint: It will install Python dependencies and Node packages, then run linters/formatters (e.g.
flake8 or black for Python, eslint for JS/TS) to enforce code quality. Any style issues cause the
build to fail early. - Run Tests: Next, it executes backend tests ( pytest ) and frontend tests ( npm test
for each app if we have any). For backend, we may use a test Supabase URL or a local Postgres (we can spin
up a service container with Postgres and provide it to tests). Our tests should cover core logic as outlined.
Code coverage can be measured to track improvement over time. - Validate Templates: We could include a
step to verify that prompt templates are valid (maybe just a simple check that no forbidden patterns or
placeholders missing – possibly using a script). For example, ensure no template has undefined

53

{{ variable }} by rendering them with dummy data. - Build Artifacts: If tests pass, the pipeline will
build the production artifacts. For the backend, that means building the Docker image. For frontends,
generating static builds. Using Turborepo, we can have a step like turbo run build --filter=... for
each app. - Security Checks: We will also enable dependency vulnerability scanning (GitHub does some by
default, and we can use tools like pip-audit or npm audit ). Also possibly run a container scan on the
built Docker image. - AI Integration Checks: Although not typical, since our development involves LLMs,
we could incorporate a step where, for example, a prompt blueprint is fed to a validation script or even a
dry-run with a small model. However, this might be out of scope for automated CI given cost and variability.
We instead rely on unit tests for logic and human review for prompt content.
The CI ensures that Claude-ready docs and Codex-generated code remain in sync: For instance, if a
developer manually changes code without updating docs or vice versa, tests might fail or at least the
difference would be noticed in code review. We can also set up a rule that the architecture spec (this
document or a derivative in docs/architecture.md ) must be updated if certain core files change (this
can be enforced lightly via PR template reminding to update docs if architecture changes).
Continuous Deployment (CD): Once CI passes on the main branch: - Backend Deployment: We use
Render.com for hosting the FastAPI service (as indicated by README). The pipeline can automatically deploy
by pushing the new Docker image or triggering Render via API. Render can auto-deploy on new commits if
configured, reading render.yaml . We'll ensure render.yaml is updated to include any new env vars
and that migrations run (maybe via a start command or a separate job). The environment variables (API
keys, etc.) are stored in Render’s dashboard, not in code, to keep them secure. We set
ENVIRONMENT=production in Render so that settings enforce required keys 153 . After deployment,
Render will start the server and the dashboard should be reachable. We include a health check endpoint
(like /metrics or a dedicated /health ) that Render can ping to ensure service up. - Frontend
Deployment: For the BrainOps admin dashboard, since it's static (Next.js exported to static files) and is
served by the backend at /dashboard/ui , we actually need to update those static files on the backend.
One approach: the Docker build process could run npm run build && npm run export for the
dashboard_ui and include the output in the image. If we keep them separate, we can host admin UI on
Vercel or Netlify as static. But embedding in backend is convenient for internal access control (the backend
already expects to serve it with auth). We'll likely bundle it with backend deployment. That means our
Dockerfile will have a step to build the Next.js admin app and copy the static output to
static/dashboard . We'll cache dependencies to not slow down build too much. For the user-facing
MyRoofGenius UI, we prefer deploying that separately on Vercel (or Netlify) since it’s a public site. The CD
pipeline can integrate with Vercel: either use Vercel’s Git integration (so any push triggers Vercel build) or
our GH Actions can do vercel deploy via token. Using Vercel’s built-in might be simpler – just ensure
environment is set there (like NEXT_PUBLIC_API_BASE if needed). The TemplateForge site (if planned
similarly) would be another Vercel project. - Database Migration: We use Alembic for migrations 154 . The
pipeline (or render start command) should run alembic upgrade head automatically so the Supabase/
Postgres schema is up to date. Supabase might not allow remote DDL by default, but since we have the
Service role key, we can run migrations via our app if configured. Possibly we might connect directly to
Supabase’s connection string to run Alembic. Alternatively, we maintain migrations for if someone uses selfhosted Postgres. In any case, apply new migrations as part of deploy. - Cron Jobs / Scheduled Tasks: If
using Render, for tasks like daily plan we have choices: (1) Rely on our internal scheduler (the
scheduler.py running inside app). (2) Use a separate cron job (Render Cron or GitHub Actions
scheduled) to hit an endpoint or run a task. We plan to have an internal scheduler, but it might rely on the
app staying awake. On Render free tier, app might spin down. But if we have constant usage or upgrade to

54

a plan, it's fine. Alternatively, configure a Render Cron job to GET our /agent/plan/daily at the scheduled
time. We'll weigh that: For reliability, maybe schedule on Render: e.g. Sunday 16:00 hit /agent/strategy/
weekly (or just rely on code). We'll incorporate whichever ensures tasks run even if no traffic. - Secrets
Management: Our backend has endpoints for storing secrets at runtime ( /secrets/store ) 155 . But in
CI/CD, we manage secrets through environment config in Render and Vercel. The BRAINSTACK_API_KEY ,
etc., should be set in Render’s env for internal use. If new secrets are needed (e.g. NOTION_API_KEY if we
implement, or PERPLEXITY cookie), those must be added to Render. We should document needed env vars
so ops can set them. (The production readiness checklist doc can list these). - Logging and Monitoring: The
deployed app will emit JSON logs to stdout (Render will capture those). We also send errors to Slack and
Supabase logs table 78 143 . So if something fails in production, the team gets alerted via Slack, and logs
are in Supabase for further analysis (could view via the dashboard UI maybe with a /logs/errors endpoint
156 which we have). For metrics, we expose /metrics for Prometheus; if we set up a Prometheus/
Grafana (maybe using Render’s managed services or something), we could monitor. If not, we can rely on
logs and Slack for now, given small scale. - Auto-Documentation: Claude is used to generate
documentation (SOPs, etc.). We might integrate that into release process for things like updating this
architecture doc or generating a changelog. For example, when merging, we could prompt Claude to
update CHANGELOG.md based on PR titles. But that's optional polish. We do have a CHANGELOG.md
already which could be manually or AI updated 157 . For now, we'll maintain it manually or via commit
messages.
Deployment of Multi-Brand Setup: The backend is multi-tenant out of the box, serving multiple brands by
data separation (project IDs, etc.). The frontends for each brand will be deployed separately (e.g.
MyRoofGenius on its domain, TemplateForge on another). They all talk to the same backend API (which
might be at api.brainstack.com or similar). We should ensure CORS allows those domains (settings can have
ALLOWED_ORIGINS ). We'll configure that (maybe allow all origins except lock down if needed). Also, rate
limiting per IP is on (100/min by slowapi default 158

80 ), which should be fine for normal usage.

Rollbacks: If something goes wrong with a deployment, Render allows redeploying an older image. We
should also keep the last stable image around. With GitHub Actions, we could push images with tags like
v1.0 etc. But since our deploy likely ties to main, we trust CI gating to catch issues. For frontends, Vercel
automatically keeps previous deployments, easy to rollback with one click if needed.
Secrets (OpenAI, etc.) Rotation: If keys need rotation, it's an ops task, but our system has the /secrets/
store to update them at runtime if needed (though presumably, restart is needed if the key is read at
startup from env – unless we code to use DB-stored keys at runtime; could consider storing API keys in
Supabase and pulling them dynamically so we can rotate without redeploying. But environment is okay for
now).
Performance & Scale: The CI ensures tests, but for scale, we rely on Render auto-scaling (we can set
concurrency for Uvicorn workers if needed, e.g. multiple workers or use Gunicorn). Celery tasks by default
run in the same dyno on Render? If heavy tasks might need separate worker dyno or use RQ/
BackgroundTasks for simplicity. The design uses Celery+Redis: we must deploy a Redis (Render provides a
free Redis or use Upstash). Connect via REDIS_URL . Ensure to configure Celery in celery_app.py .
Alternatively, given not extremely high load, FastAPI's async could handle some tasks inline for simplicity,
but let's stick to Celery for robustness with long tasks and concurrency.

55

Claude/Codex Pipeline in CI/CD: The user asked for "Claude file → test → deploy (Codex pipeline)". This
suggests perhaps a workflow where: 1. A markdown file (documentation or spec) is produced (maybe by
Claude). 2. That goes through tests (maybe a human or automated check). 3. Then Codex generates code
which is tested and deployed.
We are effectively doing that but mostly with humans writing code. If they want to integrate AI more:
Possibly they envisage a future where writing a Markdown blueprint (like this doc or smaller feature spec) in
the repo triggers an automated Codex job to create a branch with code changes. This is a bit advanced for
fully automated CI. A safer approach is a CLI or GH Action that can be manually triggered to run Codex on a
given spec file and open a PR with the changes. We can plan to include a script for that (but that might be
beyond immediate scope). However, we can certainly embed clues in our CI for any .md file changes. For
example, if docs/blueprints/new_feature.md is added, we could note in Slack that "This looks like a
spec, consider running Codex generator." Or an action that picks up special commit messages to run certain
tasks.
Given the question’s phrasing, at minimum, we ensure our pipeline connects the pieces: - For now, the
"Claude (for documentation/templates) and Codex (for code)" is a process done by developers using the
blueprint. Our pipeline tests after the code is produced by Codex manually or semi-manually.
We will document in the README how to go from blueprint to code (maybe making a script to feed
blueprint to OpenAI manually, or instruct usage of tools like GitHub Copilot with the blueprint).
Deployment Summary: - Backend on Render with Docker (with static admin UI baked in). - User Frontend
on Vercel (MyRoofGenius domain) calling backend API. - Possibly Admin Frontend also accessible via
Render (since static served) or via Vercel at a subdomain if we wanted. But internal likely fine at backend
route (with login). - DB is Supabase cloud (we supply its URL and service key to backend). - Redis for Celery
either Render or Upstash. - Slack alerts configured for errors. - Domain names configured: e.g.
api.brainstackstudio.com CNAME to Render, myroofgenius.com to Vercel, etc. - Monitoring: Slack + maybe
Sentry integration if we wanted for error tracing (we can integrate Sentry easily via their SDK for Python and
React – not asked but worth noting as future improvement).
With this CI/CD and deployment setup, each code or prompt change goes through checks and gets
deployed with minimal manual intervention, ensuring that the platform can evolve quickly and reliably. The
production readiness checklist (docs/production_checklist.md) will be updated to include verifying CI
passes, environment variables set, logging in Slack, etc., before a go-live 22 .

All components described are designed to be modular (each part can be developed or updated
independently), memory-aware (the system constantly logs and retrieves context to inform AI decisions),
scalable (using cloud services and queues to handle load), and RAG-ready (the knowledge base integration
ensures AI outputs remain grounded in our data). By following this blueprint, we set up a robust
development and deployment cycle, where Claude helps with planning and documentation, and Codex
accelerates coding, all validated by CI and deployed to cloud infrastructure seamlessly.

56

1

2

8

21

22

53

59

86

91
